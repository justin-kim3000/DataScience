{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/heewonkim/Desktop/createProgram/crawling/exam.html','r',encoding='UTF-8') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "    # find_all -> 모든 div 검색\n",
    "    all_divs = soup.find_all('div')\n",
    "    #print(all_divs)\n",
    "    # . : '\\n'을 제외한 모든 문자\n",
    "    # + : 앞 패턴이 하나 이상이어야 함\n",
    "    # ? : 앞 패턴이 없거나 하나이어야 함\n",
    "    # re.sub(패턴, 바꿀 문자열, 원본 문자열)\n",
    "    print(type(all_divs))\n",
    "    all_divs = re.sub('<.+?>','',str(all_divs)).strip()\n",
    "    #print(all_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "<p>a</p>\n",
      "<p>b</p>\n",
      "<p>c</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/heewonkim/Desktop/createProgram/crawling/exam.html','r',encoding='UTF-8') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "    # find -> 1개의 테그 정보만 검색(중복이면 첫번째 정보만 검색)\n",
    "    all_divs = soup.find('div')\n",
    "    print(all_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"ex_class\">\n",
      "<p>d</p>\n",
      "<p>e</p>\n",
      "<p>f</p>\n",
      "</div>\n",
      "<class 'bs4.element.Tag'>\n",
      "['d', '\\n', 'e', '\\n', 'f']\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/heewonkim/Desktop/createProgram/crawling/exam.html','r',encoding='UTF-8') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "    # class 검색\n",
    "    ex_class = soup.find('div', {'class':'ex_class'})\n",
    "    print(ex_class)\n",
    "    print(type(ex_class))\n",
    "    \n",
    "    ex_class = re.sub('<.+?>','',str(ex_class)).strip()\n",
    "    ex_class = list(ex_class)\n",
    "    print(ex_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"ex_id\">\n",
      "<p>g</p>\n",
      "<p>h</p>\n",
      "<p>i</p>\n",
      "</div>\n",
      "<class 'str'>\n",
      "g\n",
      "h\n",
      "i\n",
      "['g', '\\n', 'h', '\\n', 'i']\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/heewonkim/Desktop/createProgram/crawling/exam.html','r',encoding='UTF-8') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "    # id 검색\n",
    "    ex_id = soup.find('div', {'id':'ex_id'})\n",
    "    print(ex_id)    \n",
    "    ex_id = re.sub('<.+?>','',str(ex_id)).strip()\n",
    "    print(type(ex_id))\n",
    "    print(ex_id)    \n",
    "    ex_id = list(ex_id)\n",
    "    print(ex_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>a</p>, <p>b</p>, <p>c</p>, <p>d</p>, <p>e</p>, <p>f</p>, <p>g</p>, <p>h</p>, <p>i</p>, <p>python</p>, <p>bigdata</p>]\n",
      "<class 'str'>\n",
      "a, b, c, d, e, f, g, h, i, python, bigdata\n",
      "['a', ' b', ' c', ' d', ' e', ' f', ' g', ' h', ' i', ' python', ' bigdata']\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/heewonkim/Desktop/createProgram/crawling/exam.html','r',encoding='UTF-8') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "    # p 검색\n",
    "    find_p = soup.find_all('p')\n",
    "    print(find_p)    \n",
    "    find_p = re.sub('<.+?>','',str(find_p)).strip()\n",
    "    print(type(find_p))\n",
    "    find_p = find_p[1:-1]\n",
    "    #find_p=find_p.lstrip('[')\n",
    "    #find_p=find_p.rstrip(']')\n",
    "    print(find_p)\n",
    "    find_p=find_p.split(',')\n",
    "    # find_p=re.sub(',','',str(find_p))\n",
    "    print(find_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
